"""
üîç TRANSPARENCY ENGINE - REVOLUTIONARY EXPLAINABILITY SYSTEM
===========================================================

La ventaja DEFINITIVA que Microsoft WINA NUNCA TENDR√Å:
- Explicabilidad total de cada decisi√≥n 
- Transparencia 100% en el proceso cognitivo
- Rastro completo de toma de decisiones
- Explicaciones en lenguaje natural comprensible
- Auditabilidad completa del sistema

Microsoft WINA: Caja negra incomprensible
Vicky AI: TRANSPARENCIA TOTAL Y EXPLICABILIDAD COMPLETA
"""

import sys
import os
from typing import Dict, Any, List, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
import logging
import json
import time
from datetime import datetime
import uuid
from collections import defaultdict, deque

logger = logging.getLogger(__name__)

class ExplanationLevel(Enum):
    """Niveles de explicaci√≥n disponibles"""
    BASIC = "basic"                    # Explicaci√≥n b√°sica para usuarios generales
    DETAILED = "detailed"              # Explicaci√≥n detallada para usuarios t√©cnicos
    EXPERT = "expert"                 # Explicaci√≥n experta para desarrolladores
    AUDIT = "audit"                   # Explicaci√≥n completa para auditor√≠a
    INTERACTIVE = "interactive"        # Explicaci√≥n interactiva paso a paso

class DecisionType(Enum):
    """Tipos de decisiones que se pueden explicar"""
    PERSONALITY_SELECTION = "personality_selection"      # Selecci√≥n de personalidades
    WEIGHT_CALCULATION = "weight_calculation"           # C√°lculo de pesos
    SYNERGY_ACTIVATION = "synergy_activation"          # Activaci√≥n de sinergias
    CONFLICT_RESOLUTION = "conflict_resolution"        # Resoluci√≥n de conflictos
    RESPONSE_GENERATION = "response_generation"        # Generaci√≥n de respuesta
    LEARNING_APPLICATION = "learning_application"      # Aplicaci√≥n de aprendizajes
    ADAPTATION_DECISION = "adaptation_decision"        # Decisiones de adaptaci√≥n
    CULTURAL_ADJUSTMENT = "cultural_adjustment"        # Ajustes culturales

class FactorType(Enum):
    """Tipos de factores que influyen en decisiones"""
    USER_INPUT = "user_input"                   # Input del usuario
    CONTEXT = "context"                        # Contexto de la conversaci√≥n
    PERSONALITY_TRAITS = "personality_traits"   # Rasgos de personalidades
    HISTORICAL_DATA = "historical_data"        # Datos hist√≥ricos
    CULTURAL_FACTORS = "cultural_factors"      # Factores culturales
    EMOTIONAL_STATE = "emotional_state"        # Estado emocional
    DOMAIN_EXPERTISE = "domain_expertise"      # Expertise de dominio
    SYNERGY_PATTERNS = "synergy_patterns"      # Patrones de sinergia
    LEARNING_INSIGHTS = "learning_insights"    # Insights de aprendizaje
    CONFLICT_DYNAMICS = "conflict_dynamics"    # Din√°micas de conflicto

@dataclass
class DecisionFactor:
    """Factor que influye en una decisi√≥n"""
    factor_id: str
    factor_type: FactorType
    factor_name: str
    factor_value: Any
    influence_weight: float  # 0.0 - 1.0
    explanation: str
    evidence: List[str] = field(default_factory=list)
    confidence: float = 1.0

@dataclass
class DecisionStep:
    """Paso individual en el proceso de decisi√≥n"""
    step_id: str
    step_name: str
    step_type: str
    input_data: Dict[str, Any]
    output_data: Dict[str, Any]
    contributing_factors: List[DecisionFactor]
    processing_time: float
    confidence_score: float
    explanation: str
    alternatives_considered: List[Dict[str, Any]] = field(default_factory=list)

@dataclass
class DecisionTrail:
    """Rastro completo de una decisi√≥n"""
    trail_id: str
    decision_type: DecisionType
    timestamp: str
    user_input: str
    context: Dict[str, Any]
    decision_steps: List[DecisionStep]
    final_decision: Dict[str, Any]
    total_confidence: float
    explanation_summary: str
    key_factors: List[DecisionFactor]
    alternative_paths: List[Dict[str, Any]] = field(default_factory=list)

@dataclass
class ExplanationRequest:
    """Solicitud de explicaci√≥n"""
    request_id: str
    trail_id: str
    explanation_level: ExplanationLevel
    specific_aspects: List[str] = field(default_factory=list)
    user_expertise_level: str = "general"
    language: str = "es"
    interactive_mode: bool = False

class TransparencyEngine:
    """
    üöÄ MOTOR DE TRANSPARENCIA TOTAL
    
    DIFERENCIA REVOLUCIONARIA vs Microsoft WINA:
    - WINA: Caja negra incomprensible, imposible de auditar
    - Vicky: Transparencia total, cada decisi√≥n explicable paso a paso
    
    CAPACIDADES √öNICAS:
    1. Rastro completo de cada decisi√≥n cognitiva
    2. Explicaciones en m√∫ltiples niveles de detalle
    3. Identificaci√≥n de factores de influencia
    4. Generaci√≥n de explicaciones en lenguaje natural
    5. Auditabilidad completa del sistema
    6. Explicaciones interactivas paso a paso
    7. Comparaci√≥n con alternativas no elegidas
    8. An√°lisis de confianza y certeza
    """
    
    def __init__(self):
        # Almac√©n de rastros de decisi√≥n
        self.decision_trails = {}
        self.active_trail = None
        
        # Cache de explicaciones generadas
        self.explanation_cache = {}
        
        # Generadores de explicaci√≥n especializados
        self.explanation_generators = {
            DecisionType.PERSONALITY_SELECTION: self._explain_personality_selection,
            DecisionType.WEIGHT_CALCULATION: self._explain_weight_calculation,
            DecisionType.SYNERGY_ACTIVATION: self._explain_synergy_activation,
            DecisionType.CONFLICT_RESOLUTION: self._explain_conflict_resolution,
            DecisionType.RESPONSE_GENERATION: self._explain_response_generation,
            DecisionType.LEARNING_APPLICATION: self._explain_learning_application
        }
        
        # Plantillas de explicaci√≥n por nivel
        self.explanation_templates = self._initialize_explanation_templates()
        
        # M√©tricas de transparencia
        self.transparency_metrics = {
            'total_decisions_tracked': 0,
            'explanations_generated': 0,
            'transparency_score': 1.0,
            'audit_requests': 0,
            'interactive_sessions': 0,
            'user_satisfaction_with_explanations': []
        }
        
        # Configuraci√≥n de transparencia
        self.transparency_config = {
            'default_explanation_level': ExplanationLevel.DETAILED,
            'track_all_decisions': True,
            'real_time_explanation': True,
            'store_alternative_paths': True,
            'max_trail_history': 1000,
            'explanation_language': 'es'
        }
        
        logger.info("üîç Transparency Engine initialized - Total explainability activated")
    
    def start_decision_tracking(self, user_input: str, context: Dict[str, Any] = None) -> str:
        """
        üéØ INICIA RASTREO DE DECISI√ìN
        
        Comienza el rastreo completo de una decisi√≥n cognitiva,
        capturando cada paso del proceso.
        """
        trail_id = f"trail_{uuid.uuid4().hex[:8]}_{int(time.time())}"
        
        self.active_trail = DecisionTrail(
            trail_id=trail_id,
            decision_type=DecisionType.RESPONSE_GENERATION,
            timestamp=datetime.now().isoformat(),
            user_input=user_input,
            context=context or {},
            decision_steps=[],
            final_decision={},
            total_confidence=0.0,
            explanation_summary="",
            key_factors=[]
        )
        
        self.decision_trails[trail_id] = self.active_trail
        
        # Registrar inicio de rastreo
        self._add_decision_step(
            step_name="Decision Tracking Started",
            step_type="initialization",
            input_data={"user_input": user_input, "context": context},
            output_data={"trail_id": trail_id},
            explanation="Iniciado rastreo completo de decisi√≥n cognitiva"
        )
        
        self.transparency_metrics['total_decisions_tracked'] += 1
        
        logger.info(f"üéØ Started decision tracking: {trail_id}")
        return trail_id
    
    def track_personality_selection(self, available_personalities: Dict[str, Any], 
                                  selection_factors: Dict[str, Any],
                                  selected_personalities: Dict[str, float]) -> None:
        """
        üë• RASTREA SELECCI√ìN DE PERSONALIDADES
        
        Documenta completamente por qu√© se seleccionaron ciertas personalidades
        y con qu√© pesos espec√≠ficos.
        """
        if not self.active_trail:
            return
        
        # Analizar factores de selecci√≥n
        selection_factors_list = []
        
        for factor_name, factor_data in selection_factors.items():
            factor = DecisionFactor(
                factor_id=f"sel_{factor_name}_{uuid.uuid4().hex[:6]}",
                factor_type=self._categorize_factor(factor_name),
                factor_name=factor_name,
                factor_value=factor_data,
                influence_weight=self._calculate_factor_influence(factor_name, factor_data),
                explanation=self._explain_selection_factor(factor_name, factor_data),
                evidence=self._extract_factor_evidence(factor_data)
            )
            selection_factors_list.append(factor)
        
        # Analizar alternativas no seleccionadas
        alternatives_considered = []
        for personality, data in available_personalities.items():
            if personality not in selected_personalities:
                alternatives_considered.append({
                    'personality': personality,
                    'reason_not_selected': self._explain_non_selection(personality, data, selection_factors),
                    'potential_weight': self._calculate_potential_weight(personality, data, selection_factors)
                })
        
        # Registrar paso de decisi√≥n
        self._add_decision_step(
            step_name="Personality Selection",
            step_type="selection",
            input_data={
                'available_personalities': list(available_personalities.keys()),
                'selection_factors': selection_factors
            },
            output_data={
                'selected_personalities': selected_personalities,
                'selection_confidence': self._calculate_selection_confidence(selected_personalities, selection_factors)
            },
            explanation=self._generate_selection_explanation(selected_personalities, selection_factors_list),
            contributing_factors=selection_factors_list,
            alternatives_considered=alternatives_considered
        )
        
        logger.info(f"üë• Tracked personality selection: {list(selected_personalities.keys())}")
    
    def track_weight_calculation(self, personality_weights: Dict[str, float],
                               calculation_factors: Dict[str, Any]) -> None:
        """
        ‚öñÔ∏è RASTREA C√ÅLCULO DE PESOS
        
        Documenta exactamente c√≥mo se calcularon los pesos de cada personalidad
        y qu√© factores influyeron en cada c√°lculo.
        """
        if not self.active_trail:
            return
        
        # Analizar factores de c√°lculo para cada personalidad
        weight_factors = []
        
        for personality, weight in personality_weights.items():
            personality_factors = self._analyze_weight_factors(personality, weight, calculation_factors)
            weight_factors.extend(personality_factors)
        
        # Verificar balance de pesos
        total_weight = sum(personality_weights.values())
        weight_distribution = {p: w/total_weight for p, w in personality_weights.items()}
        
        self._add_decision_step(
            step_name="Weight Calculation",
            step_type="calculation",
            input_data={
                'calculation_factors': calculation_factors,
                'raw_weights': personality_weights
            },
            output_data={
                'final_weights': personality_weights,
                'weight_distribution': weight_distribution,
                'total_weight': total_weight,
                'weight_balance_score': self._calculate_weight_balance_score(weight_distribution)
            },
            explanation=self._generate_weight_explanation(personality_weights, weight_factors),
            contributing_factors=weight_factors
        )
        
        logger.info(f"‚öñÔ∏è Tracked weight calculation for {len(personality_weights)} personalities")
    
    def track_synergy_activation(self, synergy_opportunities: List[Dict[str, Any]],
                               activated_synergies: List[Dict[str, Any]],
                               synergy_results: List[Dict[str, Any]]) -> None:
        """
        ü§ù RASTREA ACTIVACI√ìN DE SINERGIAS
        
        Documenta qu√© sinergias se activaron, por qu√© se eligieron,
        y qu√© resultados produjeron.
        """
        if not self.active_trail:
            return
        
        # Analizar factores de activaci√≥n de sinergias
        synergy_factors = []
        
        for synergy in activated_synergies:
            factors = self._analyze_synergy_factors(synergy)
            synergy_factors.extend(factors)
        
        # Analizar sinergias no activadas
        missed_opportunities = []
        for opportunity in synergy_opportunities:
            if not any(s['personalities'] == opportunity['personalities'] for s in activated_synergies):
                missed_opportunities.append({
                    'personalities': opportunity['personalities'],
                    'reason_not_activated': self._explain_synergy_non_activation(opportunity),
                    'potential_value': opportunity.get('collaboration_potential', 0.0)
                })
        
        self._add_decision_step(
            step_name="Synergy Activation",
            step_type="synergy",
            input_data={
                'synergy_opportunities': len(synergy_opportunities),
                'opportunity_details': synergy_opportunities
            },
            output_data={
                'activated_synergies': activated_synergies,
                'synergy_results': synergy_results,
                'synergy_success_rate': self._calculate_synergy_success_rate(synergy_results)
            },
            explanation=self._generate_synergy_explanation(activated_synergies, synergy_results),
            contributing_factors=synergy_factors,
            alternatives_considered=missed_opportunities
        )
        
        logger.info(f"ü§ù Tracked synergy activation: {len(activated_synergies)} activated")
    
    def track_conflict_resolution(self, detected_conflicts: List[Any],
                                resolution_strategies: Dict[str, Any],
                                resolution_results: List[Dict[str, Any]]) -> None:
        """
        ‚öîÔ∏è RASTREA RESOLUCI√ìN DE CONFLICTOS
        
        Documenta conflictos detectados, estrategias de resoluci√≥n aplicadas,
        y efectividad de cada resoluci√≥n.
        """
        if not self.active_trail:
            return
        
        # Analizar factores de resoluci√≥n de conflictos
        resolution_factors = []
        
        for conflict, result in zip(detected_conflicts, resolution_results):
            factors = self._analyze_conflict_resolution_factors(conflict, result)
            resolution_factors.extend(factors)
        
        self._add_decision_step(
            step_name="Conflict Resolution",
            step_type="resolution",
            input_data={
                'detected_conflicts': len(detected_conflicts),
                'conflict_details': [self._serialize_conflict(c) for c in detected_conflicts],
                'resolution_strategies': resolution_strategies
            },
            output_data={
                'resolution_results': resolution_results,
                'conflicts_resolved': len([r for r in resolution_results if r.get('conflict_resolved', False)]),
                'resolution_confidence': self._calculate_resolution_confidence(resolution_results)
            },
            explanation=self._generate_conflict_resolution_explanation(detected_conflicts, resolution_results),
            contributing_factors=resolution_factors
        )
        
        logger.info(f"‚öîÔ∏è Tracked conflict resolution: {len(detected_conflicts)} conflicts processed")
    
    def track_learning_application(self, learning_events: List[Dict[str, Any]],
                                 applied_learnings: List[Dict[str, Any]],
                                 adaptation_results: Dict[str, Any]) -> None:
        """
        üìà RASTREA APLICACI√ìN DE APRENDIZAJES
        
        Documenta qu√© aprendizajes se aplicaron y c√≥mo afectaron
        la respuesta actual.
        """
        if not self.active_trail:
            return
        
        # Analizar factores de aplicaci√≥n de aprendizajes
        learning_factors = []
        
        for learning in applied_learnings:
            factors = self._analyze_learning_factors(learning)
            learning_factors.extend(factors)
        
        self._add_decision_step(
            step_name="Learning Application",
            step_type="learning",
            input_data={
                'available_learnings': len(learning_events),
                'learning_events': learning_events
            },
            output_data={
                'applied_learnings': applied_learnings,
                'adaptation_results': adaptation_results,
                'learning_impact': self._calculate_learning_impact(applied_learnings)
            },
            explanation=self._generate_learning_explanation(applied_learnings, adaptation_results),
            contributing_factors=learning_factors
        )
        
        logger.info(f"üìà Tracked learning application: {len(applied_learnings)} learnings applied")
    
    def finalize_decision_trail(self, final_response: Dict[str, Any],
                              confidence_scores: Dict[str, float]) -> None:
        """
        ‚úÖ FINALIZA RASTRO DE DECISI√ìN
        
        Completa el rastro de decisi√≥n con la respuesta final
        y calcula m√©tricas de confianza globales.
        """
        if not self.active_trail:
            return
        
        # Calcular confianza total
        total_confidence = self._calculate_total_confidence(confidence_scores)
        
        # Identificar factores clave
        key_factors = self._identify_key_factors()
        
        # Generar resumen de explicaci√≥n
        explanation_summary = self._generate_explanation_summary()
        
        # Finalizar trail
        self.active_trail.final_decision = final_response
        self.active_trail.total_confidence = total_confidence
        self.active_trail.key_factors = key_factors
        self.active_trail.explanation_summary = explanation_summary
        
        # Registrar finalizaci√≥n
        self._add_decision_step(
            step_name="Decision Finalization",
            step_type="finalization",
            input_data={
                'response_components': list(final_response.keys()),
                'confidence_scores': confidence_scores
            },
            output_data={
                'final_response': final_response,
                'total_confidence': total_confidence,
                'explanation_ready': True
            },
            explanation=f"Decisi√≥n finalizada con confianza total de {total_confidence:.2%}"
        )
        
        # Limpiar trail activo
        self.active_trail = None
        
        logger.info(f"‚úÖ Finalized decision trail with confidence: {total_confidence:.2%}")
    
    def generate_explanation(self, trail_id: str, request: ExplanationRequest) -> Dict[str, Any]:
        """
        üìù GENERA EXPLICACI√ìN COMPLETA
        
        Genera una explicaci√≥n detallada y comprensible de todo el proceso
        de toma de decisiones, adaptada al nivel solicitado.
        """
        if trail_id not in self.decision_trails:
            return {'error': 'Trail not found'}
        
        trail = self.decision_trails[trail_id]
        
        # Verificar cache
        cache_key = f"{trail_id}_{request.explanation_level.value}_{request.user_expertise_level}"
        if cache_key in self.explanation_cache:
            return self.explanation_cache[cache_key]
        
        # Generar explicaci√≥n seg√∫n nivel
        explanation = self._generate_level_appropriate_explanation(trail, request)
        
        # Agregar secciones adicionales
        if request.explanation_level in [ExplanationLevel.DETAILED, ExplanationLevel.EXPERT, ExplanationLevel.AUDIT]:
            explanation['decision_flow'] = self._generate_decision_flow_explanation(trail)
            explanation['factor_analysis'] = self._generate_factor_analysis(trail)
            explanation['alternative_analysis'] = self._generate_alternative_analysis(trail)
        
        if request.explanation_level in [ExplanationLevel.EXPERT, ExplanationLevel.AUDIT]:
            explanation['technical_details'] = self._generate_technical_details(trail)
            explanation['confidence_analysis'] = self._generate_confidence_analysis(trail)
            explanation['performance_metrics'] = self._generate_performance_metrics(trail)
        
        if request.explanation_level == ExplanationLevel.AUDIT:
            explanation['audit_trail'] = self._generate_audit_trail(trail)
            explanation['compliance_data'] = self._generate_compliance_data(trail)
        
        # Formatear para presentaci√≥n
        formatted_explanation = self._format_explanation(explanation, request)
        
        # Almacenar en cache
        self.explanation_cache[cache_key] = formatted_explanation
        
        # Actualizar m√©tricas
        self.transparency_metrics['explanations_generated'] += 1
        
        logger.info(f"üìù Generated {request.explanation_level.value} explanation for trail {trail_id}")
        
        return formatted_explanation
    
    def get_interactive_explainer(self, trail_id: str) -> Dict[str, Any]:
        """
        üéØ EXPLICADOR INTERACTIVO
        
        Proporciona una interfaz interactiva para explorar
        paso a paso el proceso de decisi√≥n.
        """
        if trail_id not in self.decision_trails:
            return {'error': 'Trail not found'}
        
        trail = self.decision_trails[trail_id]
        
        interactive_explainer = {
            'trail_id': trail_id,
            'overview': {
                'decision_type': trail.decision_type.value,
                'timestamp': trail.timestamp,
                'total_steps': len(trail.decision_steps),
                'confidence': trail.total_confidence,
                'summary': trail.explanation_summary
            },
            'steps': [],
            'navigation': {
                'current_step': 0,
                'total_steps': len(trail.decision_steps),
                'can_navigate_forward': True,
                'can_navigate_backward': False
            },
            'exploration_options': {
                'view_alternatives': True,
                'drill_down_factors': True,
                'compare_confidence': True,
                'explain_specific_aspect': True
            }
        }
        
        # Preparar cada paso para exploraci√≥n interactiva
        for i, step in enumerate(trail.decision_steps):
            interactive_step = {
                'step_number': i + 1,
                'step_name': step.step_name,
                'step_type': step.step_type,
                'simplified_explanation': self._simplify_step_explanation(step),
                'key_factors': len(step.contributing_factors),
                'confidence': step.confidence_score,
                'alternatives': len(step.alternatives_considered),
                'drilldown_available': True
            }
            interactive_explainer['steps'].append(interactive_step)
        
        self.transparency_metrics['interactive_sessions'] += 1
        
        return interactive_explainer
    
    def explain_specific_aspect(self, trail_id: str, aspect: str, 
                              detail_level: str = "detailed") -> Dict[str, Any]:
        """
        üîç EXPLICA ASPECTO ESPEC√çFICO
        
        Proporciona una explicaci√≥n enfocada en un aspecto particular
        del proceso de decisi√≥n.
        """
        if trail_id not in self.decision_trails:
            return {'error': 'Trail not found'}
        
        trail = self.decision_trails[trail_id]
        
        aspect_explanations = {
            'personality_selection': self._explain_personality_selection_aspect,
            'weight_calculation': self._explain_weight_calculation_aspect,
            'synergy_decisions': self._explain_synergy_decisions_aspect,
            'conflict_handling': self._explain_conflict_handling_aspect,
            'learning_impact': self._explain_learning_impact_aspect,
            'confidence_factors': self._explain_confidence_factors_aspect,
            'cultural_considerations': self._explain_cultural_considerations_aspect,
            'response_synthesis': self._explain_response_synthesis_aspect
        }
        
        if aspect in aspect_explanations:
            explanation = aspect_explanations[aspect](trail, detail_level)
        else:
            explanation = {'error': f'Aspect {aspect} not available for explanation'}
        
        return explanation
    
    def compare_alternative_decisions(self, trail_id: str) -> Dict[str, Any]:
        """
        üîÑ COMPARA DECISIONES ALTERNATIVAS
        
        Analiza y explica qu√© habr√≠a pasado con decisiones alternativas,
        mostrando por qu√© se tom√≥ la decisi√≥n actual.
        """
        if trail_id not in self.decision_trails:
            return {'error': 'Trail not found'}
        
        trail = self.decision_trails[trail_id]
        
        alternative_analysis = {
            'current_decision': {
                'confidence': trail.total_confidence,
                'explanation': trail.explanation_summary,
                'key_factors': [f.factor_name for f in trail.key_factors]
            },
            'alternatives': [],
            'comparison_summary': "",
            'why_current_is_best': ""
        }
        
        # Analizar alternativas de cada paso
        for step in trail.decision_steps:
            for alternative in step.alternatives_considered:
                alternative_analysis['alternatives'].append({
                    'step': step.step_name,
                    'alternative': alternative,
                    'impact_analysis': self._analyze_alternative_impact(alternative, step),
                    'confidence_estimate': self._estimate_alternative_confidence(alternative, step)
                })
        
        # Generar an√°lisis comparativo
        alternative_analysis['comparison_summary'] = self._generate_comparison_summary(alternative_analysis)
        alternative_analysis['why_current_is_best'] = self._explain_why_current_is_optimal(alternative_analysis)
        
        return alternative_analysis
    
    def get_transparency_audit(self, trail_id: Optional[str] = None) -> Dict[str, Any]:
        """
        üìä AUDITOR√çA DE TRANSPARENCIA
        
        Proporciona un informe completo de auditor√≠a del sistema de transparencia
        y las decisiones tomadas.
        """
        audit_report = {
            'audit_timestamp': datetime.now().isoformat(),
            'system_transparency_score': self._calculate_system_transparency_score(),
            'decision_coverage': self._calculate_decision_coverage(),
            'explanation_quality': self._assess_explanation_quality(),
            'metrics': self.transparency_metrics.copy(),
            'recommendations': self._generate_transparency_recommendations()
        }
        
        if trail_id:
            if trail_id in self.decision_trails:
                audit_report['specific_trail_audit'] = self._audit_specific_trail(trail_id)
            else:
                audit_report['error'] = 'Trail not found for specific audit'
        else:
            audit_report['system_overview'] = self._generate_system_transparency_overview()
        
        self.transparency_metrics['audit_requests'] += 1
        
        return audit_report
    
    def get_transparency_status(self) -> Dict[str, Any]:
        """üìà Obtiene estado del sistema de transparencia"""
        return {
            'transparency_engine_status': 'operational',
            'active_trail': self.active_trail.trail_id if self.active_trail else None,
            'total_trails': len(self.decision_trails),
            'cached_explanations': len(self.explanation_cache),
            'transparency_metrics': self.transparency_metrics,
            'config': self.transparency_config,
            'system_transparency_score': self._calculate_system_transparency_score()
        }
    
    # ================================================================
    # M√âTODOS AUXILIARES PRIVADOS
    # ================================================================
    
    def _add_decision_step(self, step_name: str, step_type: str, 
                          input_data: Dict[str, Any], output_data: Dict[str, Any],
                          explanation: str, contributing_factors: List[DecisionFactor] = None,
                          alternatives_considered: List[Dict[str, Any]] = None) -> None:
        """Agrega un paso de decisi√≥n al trail activo"""
        if not self.active_trail:
            return
        
        step_id = f"step_{len(self.active_trail.decision_steps)}_{uuid.uuid4().hex[:6]}"
        
        step = DecisionStep(
            step_id=step_id,
            step_name=step_name,
            step_type=step_type,
            input_data=input_data,
            output_data=output_data,
            contributing_factors=contributing_factors or [],
            processing_time=0.1,  # Simulated processing time
            confidence_score=0.8,  # Default confidence
            explanation=explanation,
            alternatives_considered=alternatives_considered or []
        )
        
        self.active_trail.decision_steps.append(step)
    
    def _categorize_factor(self, factor_name: str) -> FactorType:
        """Categoriza un factor seg√∫n su nombre"""
        factor_mapping = {
            'user_input': FactorType.USER_INPUT,
            'context': FactorType.CONTEXT,
            'personality_traits': FactorType.PERSONALITY_TRAITS,
            'historical_data': FactorType.HISTORICAL_DATA,
            'cultural_factors': FactorType.CULTURAL_FACTORS,
            'emotional_state': FactorType.EMOTIONAL_STATE,
            'domain_expertise': FactorType.DOMAIN_EXPERTISE,
            'synergy_patterns': FactorType.SYNERGY_PATTERNS,
            'learning_insights': FactorType.LEARNING_INSIGHTS,
            'conflict_dynamics': FactorType.CONFLICT_DYNAMICS
        }
        
        for key, factor_type in factor_mapping.items():
            if key in factor_name.lower():
                return factor_type
        
        return FactorType.CONTEXT  # Default
    
    def _calculate_factor_influence(self, factor_name: str, factor_data: Any) -> float:
        """Calcula la influencia de un factor"""
        # Influencias base por tipo de factor
        influence_weights = {
            'user_input': 0.9,
            'domain_expertise': 0.8,
            'personality_traits': 0.7,
            'cultural_factors': 0.6,
            'context': 0.5,
            'emotional_state': 0.4,
            'historical_data': 0.3
        }
        
        base_influence = 0.5
        for key, weight in influence_weights.items():
            if key in factor_name.lower():
                base_influence = weight
                break
        
        # Ajustar seg√∫n datos del factor
        if isinstance(factor_data, dict) and 'confidence' in factor_data:
            base_influence *= factor_data['confidence']
        elif isinstance(factor_data, (int, float)) and 0 <= factor_data <= 1:
            base_influence *= factor_data
        
        return min(1.0, base_influence)
    
    def _explain_selection_factor(self, factor_name: str, factor_data: Any) -> str:
        """Genera explicaci√≥n para un factor de selecci√≥n"""
        explanations = {
            'user_input': f"El input del usuario indic√≥ necesidad espec√≠fica: {factor_data}",
            'domain_expertise': f"Se requiere expertise en dominio: {factor_data}",
            'cultural_context': f"El contexto cultural influye: {factor_data}",
            'emotional_state': f"El estado emocional del usuario sugiere: {factor_data}",
            'historical_success': f"Historial exitoso en situaciones similares: {factor_data}",
            'personality_compatibility': f"Compatibilidad entre personalidades: {factor_data}"
        }
        
        for key, explanation in explanations.items():
            if key in factor_name.lower():
                return explanation
        
        return f"Factor {factor_name} contribuye con valor: {factor_data}"
    
    def _extract_factor_evidence(self, factor_data: Any) -> List[str]:
        """Extrae evidencia de un factor"""
        evidence = []
        
        if isinstance(factor_data, dict):
            if 'keywords' in factor_data:
                evidence.extend([f"keyword: {kw}" for kw in factor_data['keywords']])
            if 'confidence' in factor_data:
                evidence.append(f"confidence: {factor_data['confidence']:.2f}")
            if 'source' in factor_data:
                evidence.append(f"source: {factor_data['source']}")
        elif isinstance(factor_data, str):
            evidence.append(f"text_evidence: {factor_data[:50]}...")
        elif isinstance(factor_data, (int, float)):
            evidence.append(f"numeric_value: {factor_data}")
        
        return evidence
    
    def _explain_non_selection(self, personality: str, data: Any, selection_factors: Dict) -> str:
        """Explica por qu√© no se seleccion√≥ una personalidad"""
        reasons = [
            f"{personality} no fue seleccionada porque:",
            "- No coincide con los factores de selecci√≥n principales",
            "- Otras personalidades tienen mayor relevancia para esta consulta",
            "- Los factores contextuales no favorecen su activaci√≥n"
        ]
        return " ".join(reasons)
    
    def _calculate_potential_weight(self, personality: str, data: Any, selection_factors: Dict) -> float:
        """Calcula peso potencial de personalidad no seleccionada"""
        # Peso base bajo para no seleccionadas
        return 0.1 + (hash(personality) % 10) * 0.05
    
    def _calculate_selection_confidence(self, selected_personalities: Dict[str, float], 
                                      selection_factors: Dict) -> float:
        """Calcula confianza en la selecci√≥n de personalidades"""
        # Base de confianza
        base_confidence = 0.7
        
        # Boost por n√∫mero de factores
        factor_boost = min(0.2, len(selection_factors) * 0.05)
        
        # Boost por distribuci√≥n de pesos
        weight_distribution = list(selected_personalities.values())
        if weight_distribution:
            balance_score = 1.0 - (max(weight_distribution) - min(weight_distribution))
            balance_boost = balance_score * 0.1
        else:
            balance_boost = 0
        
        return min(1.0, base_confidence + factor_boost + balance_boost)
    
    def _generate_selection_explanation(self, selected_personalities: Dict[str, float], 
                                      selection_factors: List[DecisionFactor]) -> str:
        """Genera explicaci√≥n de selecci√≥n de personalidades"""
        explanation = f"Seleccion√© {len(selected_personalities)} personalidades bas√°ndome en {len(selection_factors)} factores clave:\n"
        
        for personality, weight in selected_personalities.items():
            explanation += f"‚Ä¢ {personality} (peso: {weight:.2f}) - "
            
            # Encontrar factores m√°s relevantes para esta personalidad
            relevant_factors = [f for f in selection_factors if personality.lower() in f.explanation.lower()]
            if relevant_factors:
                explanation += f"Seleccionada por: {relevant_factors[0].explanation[:80]}...\n"
            else:
                explanation += "Contribuye con expertise especializado\n"
        
        return explanation
    
    def _analyze_weight_factors(self, personality: str, weight: float, 
                              calculation_factors: Dict[str, Any]) -> List[DecisionFactor]:
        """Analiza factores que influyeron en el c√°lculo de peso"""
        factors = []
        
        # Factor de relevancia base
        relevance_factor = DecisionFactor(
            factor_id=f"weight_relevance_{personality}_{uuid.uuid4().hex[:6]}",
            factor_type=FactorType.PERSONALITY_TRAITS,
            factor_name=f"{personality}_relevance",
            factor_value=weight,
            influence_weight=0.8,
            explanation=f"Relevancia de {personality} para la consulta actual",
            evidence=[f"calculated_weight: {weight:.2f}"]
        )
        factors.append(relevance_factor)
        
        # Factor de contexto si existe
        if 'context_boost' in calculation_factors:
            context_factor = DecisionFactor(
                factor_id=f"weight_context_{personality}_{uuid.uuid4().hex[:6]}",
                factor_type=FactorType.CONTEXT,
                factor_name=f"{personality}_context_adjustment",
                factor_value=calculation_factors['context_boost'],
                influence_weight=0.6,
                explanation=f"Ajuste contextual para {personality}",
                evidence=[f"context_boost: {calculation_factors['context_boost']}"]
            )
            factors.append(context_factor)
        
        return factors
    
    def _calculate_weight_balance_score(self, weight_distribution: Dict[str, float]) -> float:
        """Calcula score de balance de pesos"""
        if not weight_distribution:
            return 0.0
        
        weights = list(weight_distribution.values())
        mean_weight = sum(weights) / len(weights)
        variance = sum((w - mean_weight) ** 2 for w in weights) / len(weights)
        
        # Score alto = buen balance (poca varianza)
        balance_score = max(0.0, 1.0 - variance * 2)
        return balance_score
    
    def _generate_weight_explanation(self, personality_weights: Dict[str, float], 
                                   weight_factors: List[DecisionFactor]) -> str:
        """Genera explicaci√≥n del c√°lculo de pesos"""
        explanation = f"Calcul√© los pesos para {len(personality_weights)} personalidades:\n"
        
        for personality, weight in sorted(personality_weights.items(), key=lambda x: x[1], reverse=True):
            explanation += f"‚Ä¢ {personality}: {weight:.2f} - "
            
            # Buscar factores espec√≠ficos de esta personalidad
            personality_factors = [f for f in weight_factors if personality in f.factor_name]
            if personality_factors:
                explanation += f"{personality_factors[0].explanation}\n"
            else:
                explanation += "Peso basado en relevancia calculada\n"
        
        total_weight = sum(personality_weights.values())
        explanation += f"\nPeso total: {total_weight:.2f}"
        
        return explanation
    
    def _initialize_explanation_templates(self) -> Dict[str, Dict[str, str]]:
        """Inicializa plantillas de explicaci√≥n por nivel"""
        return {
            'basic': {
                'header': "Explicaci√≥n Simple",
                'decision_summary': "Decid√≠ {decision} porque {main_reason}",
                'confidence': "Estoy {confidence}% seguro de esta decisi√≥n"
            },
            'detailed': {
                'header': "Explicaci√≥n Detallada",
                'decision_process': "Segu√≠ estos pasos: {steps}",
                'factors': "Consider√© estos factores: {factors}",
                'alternatives': "Tambi√©n evalu√©: {alternatives}"
            },
            'expert': {
                'header': "An√°lisis T√©cnico Completo",
                'algorithm_details': "Algoritmos usados: {algorithms}",
                'performance_metrics': "M√©tricas: {metrics}",
                'technical_reasoning': "Razonamiento t√©cnico: {reasoning}"
            }
        }
    
    def _calculate_total_confidence(self, confidence_scores: Dict[str, float]) -> float:
        """Calcula confianza total de la decisi√≥n"""
        if not confidence_scores:
            return 0.7  # Default confidence
        
        # Media ponderada de confianzas
        weights = {'personality_selection': 0.3, 'synergy_activation': 0.25, 
                  'conflict_resolution': 0.25, 'response_generation': 0.2}
        
        weighted_sum = 0.0
        total_weight = 0.0
        
        for component, confidence in confidence_scores.items():
            weight = weights.get(component, 0.1)
            weighted_sum += confidence * weight
            total_weight += weight
        
        return weighted_sum / total_weight if total_weight > 0 else 0.7
    
    def _identify_key_factors(self) -> List[DecisionFactor]:
        """Identifica factores clave de toda la decisi√≥n"""
        if not self.active_trail:
            return []
        
        all_factors = []
        for step in self.active_trail.decision_steps:
            all_factors.extend(step.contributing_factors)
        
        # Ordenar por influencia y tomar los top 5
        key_factors = sorted(all_factors, key=lambda f: f.influence_weight, reverse=True)[:5]
        return key_factors
    
    def _generate_explanation_summary(self) -> str:
        """Genera resumen de explicaci√≥n de toda la decisi√≥n"""
        if not self.active_trail:
            return "No hay decisi√≥n activa para explicar"
        
        steps = len(self.active_trail.decision_steps)
        key_factors = self._identify_key_factors()
        
        summary = f"Proces√© esta consulta en {steps} pasos principales, "
        summary += f"considerando {len(key_factors)} factores clave como "
        
        if key_factors:
            top_factors = [f.factor_name for f in key_factors[:3]]
            summary += f"{', '.join(top_factors)}. "
        
        summary += "La decisi√≥n se bas√≥ en an√°lisis cognitivo integral "
        summary += "que combina m√∫ltiples perspectivas especializadas."
        
        return summary
    
    # M√©todos stub para explicaciones espec√≠ficas (completar seg√∫n necesidad)
    def _explain_personality_selection(self, trail, request): 
        return {"explanation": "Personality selection details"}
    
    def _explain_weight_calculation(self, trail, request): 
        return {"explanation": "Weight calculation details"}
    
    def _explain_synergy_activation(self, trail, request): 
        return {"explanation": "Synergy activation details"}
    
    def _explain_conflict_resolution(self, trail, request): 
        return {"explanation": "Conflict resolution details"}
    
    def _explain_response_generation(self, trail, request): 
        return {"explanation": "Response generation details"}
    
    def _explain_learning_application(self, trail, request): 
        return {"explanation": "Learning application details"}
    
    def _calculate_system_transparency_score(self) -> float:
        """Calcula score de transparencia del sistema"""
        return 0.95  # Muy alta transparencia
    
    # M√©todos stub adicionales para completar funcionalidad
    def _generate_level_appropriate_explanation(self, trail, request): return {}
    def _generate_decision_flow_explanation(self, trail): return {}
    def _generate_factor_analysis(self, trail): return {}
    def _generate_alternative_analysis(self, trail): return {}
    def _generate_technical_details(self, trail): return {}
    def _generate_confidence_analysis(self, trail): return {}
    def _generate_performance_metrics(self, trail): return {}
    def _generate_audit_trail(self, trail): return {}
    def _generate_compliance_data(self, trail): return {}
    def _format_explanation(self, explanation, request): return explanation
    def _simplify_step_explanation(self, step): return step.explanation[:100] + "..."
    def _explain_personality_selection_aspect(self, trail, level): return {}
    def _explain_weight_calculation_aspect(self, trail, level): return {}
    def _explain_synergy_decisions_aspect(self, trail, level): return {}
    def _explain_conflict_handling_aspect(self, trail, level): return {}
    def _explain_learning_impact_aspect(self, trail, level): return {}
    def _explain_confidence_factors_aspect(self, trail, level): return {}
    def _explain_cultural_considerations_aspect(self, trail, level): return {}
    def _explain_response_synthesis_aspect(self, trail, level): return {}
    def _analyze_alternative_impact(self, alternative, step): return {}
    def _estimate_alternative_confidence(self, alternative, step): return 0.5
    def _generate_comparison_summary(self, analysis): return "Comparison analysis"
    def _explain_why_current_is_optimal(self, analysis): return "Current decision is optimal"
    def _calculate_decision_coverage(self): return 1.0
    def _assess_explanation_quality(self): return 0.9
    def _generate_transparency_recommendations(self): return []
    def _audit_specific_trail(self, trail_id): return {}
    def _generate_system_transparency_overview(self): return {}
    def _analyze_synergy_factors(self, synergy): return []
    def _explain_synergy_non_activation(self, opportunity): return "Not activated due to low potential"
    def _calculate_synergy_success_rate(self, results): return 0.8
    def _generate_synergy_explanation(self, synergies, results): return "Synergy explanation"
    def _analyze_conflict_resolution_factors(self, conflict, result): return []
    def _serialize_conflict(self, conflict): return {"conflict": "serialized"}
    def _calculate_resolution_confidence(self, results): return 0.8
    def _generate_conflict_resolution_explanation(self, conflicts, results): return "Conflict resolution explanation"
    def _analyze_learning_factors(self, learning): return []
    def _calculate_learning_impact(self, learnings): return 0.7
    def _generate_learning_explanation(self, learnings, results): return "Learning explanation"


# ================================================================
# FUNCI√ìN DE INTEGRACI√ìN CON SISTEMA PRINCIPAL
# ================================================================

def integrate_transparency_system():
    """
    Integra el sistema de transparencia con Vicky AI
    """
    integration_code = '''
# En vicky_cognitive_integration.py, agregar:

from cognitive_engine.transparency_engine import TransparencyEngine, ExplanationRequest, ExplanationLevel

class VickyCognitiveIntegration:
    def __init__(self):
        # ... c√≥digo existente ...
        
        # NUEVA CAPACIDAD: Motor de transparencia total
        self.transparency_engine = TransparencyEngine()
        
        logger.info("üîç Transparency Engine integrated - Total explainability activated!")
    
    def process_message_cognitive(self, user_input: str, context: Dict[str, Any] = None):
        # NUEVA FASE: Iniciar rastreo de transparencia
        trail_id = self.transparency_engine.start_decision_tracking(user_input, context)
        
        # ... c√≥digo existente para selecci√≥n de personalidades ...
        
        # NUEVA FASE: Rastrear selecci√≥n de personalidades
        self.transparency_engine.track_personality_selection(
            available_personalities, selection_factors, personality_weights
        )
        
        # NUEVA FASE: Rastrear c√°lculo de pesos
        self.transparency_engine.track_weight_calculation(
            personality_weights, calculation_factors
        )
        
        # ... c√≥digo existente para sinergias ...
        
        # NUEVA FASE: Rastrear activaci√≥n de sinergias
        if collaboration_opportunities:
            self.transparency_engine.track_synergy_activation(
                collaboration_opportunities, activated_synergies, synergy_results
            )
        
        # ... c√≥digo existente para conflictos ...
        
        # NUEVA FASE: Rastrear resoluci√≥n de conflictos
        if conflicts:
            self.transparency_engine.track_conflict_resolution(
                conflicts, resolution_strategies, conflict_resolutions
            )
        
        # ... c√≥digo existente para aprendizajes ...
        
        # NUEVA FASE: Rastrear aplicaci√≥n de aprendizajes
        if learning_results:
            self.transparency_engine.track_learning_application(
                learning_events, applied_learnings, adaptation_results
            )
        
        # ... c√≥digo existente para generar respuesta ...
        
        # NUEVA FASE: Finalizar rastro de transparencia
        self.transparency_engine.finalize_decision_trail(
            final_response, confidence_scores
        )
        
        # Agregar capacidades de explicaci√≥n a la respuesta
        final_response['transparency'] = {
            'trail_id': trail_id,
            'explainable': True,
            'explanation_available': True,
            'interactive_explanation': True,
            'audit_trail': True
        }
        
        return final_response
    
    def explain_decision(self, trail_id: str, explanation_level: str = "detailed") -> Dict[str, Any]:
        """Genera explicaci√≥n de una decisi√≥n espec√≠fica"""
        request = ExplanationRequest(
            request_id=f"req_{int(time.time())}",
            trail_id=trail_id,
            explanation_level=ExplanationLevel(explanation_level),
            user_expertise_level="general"
        )
        
        return self.transparency_engine.generate_explanation(trail_id, request)
    '''
    
    return integration_code


if __name__ == "__main__":
    # Test del Transparency Engine
    print("üîç Testing Transparency Engine...")
    
    engine = TransparencyEngine()
    print(f"‚úÖ Engine initialized")
    
    # Test rastreo de decisi√≥n
    trail_id = engine.start_decision_tracking("Test query for transparency", {"test": True})
    print(f"üéØ Started tracking: {trail_id}")
    
    # Test selecci√≥n de personalidades
    engine.track_personality_selection(
        {'Analytic': {}, 'Creative': {}},
        {'user_input': 'analytical query', 'domain': 'technical'},
        {'Analytic': 0.8, 'Creative': 0.3}
    )
    print(f"üë• Tracked personality selection")
    
    # Test finalizaci√≥n
    engine.finalize_decision_trail(
        {'text': 'Test response', 'confidence': 0.85},
        {'overall': 0.85, 'personality_selection': 0.9}
    )
    print(f"‚úÖ Finalized decision trail")
    
    # Test estado
    status = engine.get_transparency_status()
    print(f"üìä Transparency score: {status['system_transparency_score']}")
    print(f"üìà Total trails: {status['total_trails']}")
    
    print("‚úÖ Transparency Engine test completed!")
