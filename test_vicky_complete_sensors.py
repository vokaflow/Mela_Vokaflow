#!/usr/bin/env python3
"""
Prueba completa de integraciÃ³n sensorial de Vicky
OÃDO + VISTA + VOZ + KINECT integrados con cerebro dual
"""

import asyncio
import time
import logging
from src.vicky.sensors import VickySensoryIntegration
from scripts.kinect_integration import KinectAPI

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("test_vicky_complete")

async def test_complete_sensory_integration():
    """Prueba la integraciÃ³n sensorial completa de Vicky"""
    
    print("ðŸŽ­ FASE 2: INTEGRACIÃ“N SENSORIAL COMPLETA")
    print("=" * 50)
    
    # 1. Inicializar Kinect
    print("\nðŸ¤– Inicializando Kinect...")
    kinect = KinectAPI(simulation_mode=True)  # Usar modo simulaciÃ³n para pruebas
    kinect.initialize()
    kinect_status = kinect.get_status()
    print(f"âœ… Kinect: {kinect_status}")
    
    # 2. Inicializar integraciÃ³n sensorial
    print("\nðŸŽ­ Inicializando integraciÃ³n sensorial de Vicky...")
    vicky_sensors = VickySensoryIntegration(kinect)
    
    # 3. Configurar callbacks de monitoreo
    def on_interaction(interaction_type, input_text, output_text, timestamp):
        print(f"ðŸ’¬ INTERACCIÃ“N [{interaction_type}]: '{input_text}' -> '{output_text}'")
    
    def on_state_change(change_type, state, timestamp):
        print(f"ðŸ”„ ESTADO CAMBIÃ“ [{change_type}]: Activada={state.is_activated}, Personas={state.faces_detected}")
    
    vicky_sensors.set_interaction_callback(on_interaction)
    vicky_sensors.set_state_change_callback(on_state_change)
    
    # 4. Iniciar todos los sensores
    print("\nðŸš€ Iniciando TODOS los sensores de Vicky...")
    await vicky_sensors.start_all_sensors()
    
    # 5. Mostrar estado inicial
    print("\nðŸ“Š ESTADO INICIAL:")
    status = vicky_sensors.get_comprehensive_status()
    print_status_summary(status)
    
    # 6. Simular interacciones durante 30 segundos
    print("\nðŸŽ¬ Simulando interacciones por 30 segundos...")
    print("   (En un sistema real, aquÃ­ Vicky estarÃ­a escuchando y viendo)")
    
    for i in range(30):
        # Mostrar estado cada 5 segundos
        if i % 5 == 0:
            print(f"\nâ±ï¸  SEGUNDO {i}/30:")
            current_status = vicky_sensors.get_comprehensive_status()
            
            # Mostrar informaciÃ³n clave
            sensory = current_status["sensory_state"]
            audio = current_status["audio_status"]
            vision = current_status["vision_status"]
            
            print(f"   ðŸ‘‚ Audio: Escuchando={audio['is_listening']}, Activado={audio['is_activated']}")
            print(f"   ðŸ‘€ VisiÃ³n: Procesando={vision['is_processing']}, Caras={vision['faces_detected']}")
            print(f"   ðŸ§  Estado: Persona presente={sensory['person_present']}, Mood={sensory['current_mood']}")
            
        await asyncio.sleep(1)
    
    # 7. Detener sensores
    print("\nðŸ›‘ Deteniendo todos los sensores...")
    vicky_sensors.stop_all_sensors()
    
    # 8. Estado final
    print("\nðŸ“Š ESTADO FINAL:")
    final_status = vicky_sensors.get_comprehensive_status()
    print_status_summary(final_status)
    
    print("\nâœ… PRUEBA COMPLETA FINALIZADA")
    print("ðŸŽ‰ Vicky ahora tiene OÃDO, VISTA y VOZ completamente integrados!")

def print_status_summary(status):
    """Imprime resumen del estado"""
    sensory = status["sensory_state"]
    audio = status["audio_status"]
    vision = status["vision_status"]
    brain = status["brain_status"]
    kinect = status["kinect_status"]
    config = status["configuration"]
    
    print(f"   ðŸŽ­ SENSORES:")
    print(f"      ðŸ‘‚ OÃ­do: {'ðŸŸ¢ Activo' if audio['is_listening'] else 'ðŸ”´ Inactivo'}")
    print(f"      ðŸ‘€ Vista: {'ðŸŸ¢ Activo' if vision['is_processing'] else 'ðŸ”´ Inactivo'}")
    print(f"      ðŸŽµ Voz: {'ðŸŸ¢ Disponible' if 'tts_model' in str(brain) else 'ðŸŸ¢ Disponible'}")
    print(f"      ðŸ¤– Kinect: {'ðŸŸ¢ Conectado' if kinect['connected'] else 'ðŸŸ¡ Simulado'}")
    
    print(f"   ðŸ§  CEREBRO DUAL:")
    print(f"      Balance: {brain['hemisphere_balance']}")
    print(f"      Peticiones: {brain.get('total_requests', 0)}")
    
    print(f"   ðŸ‘¤ DETECCIÃ“N:")
    print(f"      Personas presentes: {sensory['faces_detected']}")
    print(f"      Mood actual: {sensory['current_mood']}")
    print(f"      Activada: {'ðŸ”¥ SÃ' if sensory['is_activated'] else 'ðŸ’¤ NO'}")
    
    print(f"   âš™ï¸  CONFIGURACIÃ“N:")
    print(f"      Auto-orientaciÃ³n: {'âœ…' if config['auto_orient_to_person'] else 'âŒ'}")
    print(f"      Respuestas emocionales: {'âœ…' if config['emotional_response_enabled'] else 'âŒ'}")
    print(f"      Timeout conversaciÃ³n: {config['conversation_timeout']}s")
    
    print(f"   ðŸ“ˆ ESTADÃSTICAS:")
    print(f"      Interacciones registradas: {status['interaction_history_count']}")
    print(f"      Ãšltima interacciÃ³n: {time.ctime(sensory['last_interaction']) if sensory['last_interaction'] > 0 else 'Nunca'}")

if __name__ == "__main__":
    asyncio.run(test_complete_sensory_integration()) 