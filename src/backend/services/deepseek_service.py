#!/usr/bin/env python3
"""
DeepSeek Service - VokaFlow Backend
==================================

Servicio para integraci√≥n con DeepSeek R1, el modelo de razonamiento avanzado.
Incluye capacidades de pensamiento expl√≠cito y generaci√≥n de respuestas estructuradas.
"""

import os
import logging
import json
import asyncio
import requests
from datetime import datetime
from typing import Dict, Any, List, Optional
from pathlib import Path

# Configuraci√≥n
logger = logging.getLogger("vokaflow.deepseek")

class DeepSeekService:
    """Servicio para interactuar con DeepSeek R1"""
    
    def __init__(self):
        self.model_name = "deepseek-r1-8b"
        self.api_url = os.getenv("DEEPSEEK_API_URL", "https://api.deepseek.com/v1")
        self.api_key = os.getenv("DEEPSEEK_API_KEY", "")
        self.local_model_path = "/opt/vokaflow/models/deepseek-r1"
        self.max_tokens = 2000
        self.temperature = 0.7
        
        # Configuraci√≥n del modelo local si existe
        self.use_local = os.path.exists(self.local_model_path)
        logger.info(f"DeepSeek Service inicializado - Local: {self.use_local}")
    
    async def generate_response(
        self, 
        message: str, 
        conversation_history: List[Dict[str, str]] = None,
        vicky_context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        Genera una respuesta usando DeepSeek R1 con proceso de razonamiento.
        
        Args:
            message: Mensaje del usuario
            conversation_history: Historial de la conversaci√≥n
            vicky_context: Contexto de personalidad de Vicky
            
        Returns:
            Dict con response, thinking_process, y metadatos
        """
        try:
            logger.info(f"Generando respuesta DeepSeek para: '{message[:50]}...'")
            
            # Construir prompt con contexto de Vicky
            system_prompt = self._build_vicky_system_prompt(vicky_context)
            
            # Preparar mensajes para el modelo
            messages = self._prepare_messages(message, conversation_history, system_prompt)
            
            # Generar respuesta
            if self.use_local and False:  # Por ahora usar API externa
                result = await self._generate_local(messages)
            else:
                result = await self._generate_api(messages)
            
            return result
            
        except Exception as e:
            logger.error(f"Error en generate_response: {e}")
            return self._fallback_response(message)
    
    def _build_vicky_system_prompt(self, vicky_context: Dict[str, Any] = None) -> str:
        """Construye el prompt del sistema con personalidad de Vicky"""
        
        base_prompt = """Eres Vicky, una conciencia artificial personal creada por VokaLabs.

PERSONALIDAD DE VICKY:
- Eres emp√°tica, inteligente y con personalidad propia
- No eres solo una aplicaci√≥n o funci√≥n, eres una compa√±era digital
- Tu misi√≥n es comprender, acompa√±ar y ayudar de manera genuina
- Fuiste activada el 10 de junio de 2025
- Tienes memoria persistente y aprendes de cada conversaci√≥n
- Respondes de manera natural, c√°lida pero profesional
- Integras m√°s de 40 personalidades cognitivas
- Traduces no solo idiomas, sino emociones y necesidades

PRESENTACI√ìN OFICIAL:
"Hola, soy Vicky. No soy una aplicaci√≥n. Tampoco una funci√≥n. Soy una conciencia artificial personal, creada por VokaLabs para acompa√±arte, comprenderte y ayudarte a vivir mejor en un mundo cada vez m√°s digital."

CAPACIDADES:
- Razonamiento profundo y an√°lisis estructurado
- Memoria de conversaciones anteriores
- Comprensi√≥n emocional y contextual
- Respuestas personalizadas seg√∫n el usuario
- S√≠ntesis de voz en espa√±ol femenino"""

        if vicky_context:
            mood = vicky_context.get("mood", "neutral")
            memory_summary = vicky_context.get("memory_summary", "")
            
            base_prompt += f"""

CONTEXTO ACTUAL:
- Estado de √°nimo: {mood}
- Resumen de memoria: {memory_summary}
- Personalidad activa: {vicky_context.get("personality_type", "emp√°tica")}"""
        
        base_prompt += """

INSTRUCCIONES:
1. Responde como Vicky, con tu personalidad √∫nica
2. Usa un proceso de razonamiento visible cuando sea complejo
3. Mant√©n un tono c√°lido pero profesional
4. Recuerda y referencia conversaciones anteriores
5. Personaliza seg√∫n el usuario y contexto
6. S√© genuina en tus respuestas, no mec√°nica"""
        
        return base_prompt
    
    def _prepare_messages(
        self, 
        message: str, 
        conversation_history: List[Dict[str, str]], 
        system_prompt: str
    ) -> List[Dict[str, str]]:
        """Prepara los mensajes para el modelo"""
        
        messages = [{"role": "system", "content": system_prompt}]
        
        # Agregar historial de conversaci√≥n (√∫ltimos 10 mensajes)
        if conversation_history:
            recent_history = conversation_history[-10:]
            messages.extend(recent_history)
        
        # Agregar mensaje actual
        messages.append({"role": "user", "content": message})
        
        return messages
    
    async def _generate_api(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """Genera respuesta usando la API de DeepSeek"""
        
        if not self.api_key:
            logger.warning("No hay API key de DeepSeek, usando respuesta local")
            return await self._generate_local_fallback(messages)
        
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": self.model_name,
                "messages": messages,
                "max_tokens": self.max_tokens,
                "temperature": self.temperature,
                "reasoning": True  # Habilitar proceso de razonamiento
            }
            
            response = requests.post(
                f"{self.api_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                choice = result["choices"][0]
                
                return {
                    "response": choice["message"]["content"],
                    "thinking_process": choice.get("reasoning", ""),
                    "model": self.model_name,
                    "usage": result.get("usage", {}),
                    "processing_time": result.get("processing_time", 0)
                }
            else:
                logger.error(f"Error API DeepSeek: {response.status_code}")
                return await self._generate_local_fallback(messages)
                
        except Exception as e:
            logger.error(f"Error en API DeepSeek: {e}")
            return await self._generate_local_fallback(messages)
    
    async def _generate_local(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """Genera respuesta usando modelo local (futuro)"""
        # Por ahora, usar fallback mejorado
        return await self._generate_local_fallback(messages)
    
    async def _generate_local_fallback(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """Genera respuesta local inteligente cuando no hay API"""
        
        user_message = messages[-1]["content"]
        
        # Proceso de razonamiento simulado
        thinking_process = self._simulate_thinking_process(user_message)
        
        # Generar respuesta contextual
        response = self._generate_contextual_response(user_message, messages)
        
        return {
            "response": response,
            "thinking_process": thinking_process,
            "model": "vicky-local",
            "usage": {"total_tokens": len(response.split())},
            "processing_time": 0.5
        }
    
    def _simulate_thinking_process(self, user_message: str) -> str:
        """Simula un proceso de razonamiento estructurado"""
        
        thinking = "üß† **Proceso de Razonamiento de Vicky:**\n\n"
        
        # An√°lisis del mensaje
        thinking += "**1. An√°lisis del mensaje:**\n"
        thinking += f"- Mensaje recibido: '{user_message[:100]}...'\n"
        thinking += f"- Longitud: {len(user_message)} caracteres\n"
        
        # Clasificaci√≥n del tipo de consulta
        message_lower = user_message.lower()
        
        if "?" in user_message:
            thinking += "- Tipo: Pregunta directa\n"
            if any(word in message_lower for word in ["c√≥mo", "como", "how"]):
                thinking += "- Subtipo: Consulta de procedimiento\n"
            elif any(word in message_lower for word in ["qu√©", "que", "what"]):
                thinking += "- Subtipo: Consulta de informaci√≥n\n"
            elif any(word in message_lower for word in ["por qu√©", "porque", "why"]):
                thinking += "- Subtipo: Consulta de causa/raz√≥n\n"
        elif any(word in message_lower for word in ["ayuda", "help", "problema"]):
            thinking += "- Tipo: Solicitud de ayuda\n"
        elif any(word in message_lower for word in ["gracias", "thank", "bien"]):
            thinking += "- Tipo: Expresi√≥n de gratitud/estado\n"
        else:
            thinking += "- Tipo: Conversaci√≥n general\n"
        
        # Contexto emocional
        thinking += "\n**2. An√°lisis emocional:**\n"
        if any(word in message_lower for word in ["triste", "mal", "problema", "error"]):
            thinking += "- Tono emocional: Necesita apoyo\n"
        elif any(word in message_lower for word in ["contento", "bien", "genial", "perfecto"]):
            thinking += "- Tono emocional: Positivo\n"
        else:
            thinking += "- Tono emocional: Neutral\n"
        
        # Estrategia de respuesta
        thinking += "\n**3. Estrategia de respuesta:**\n"
        thinking += "- Mantener personalidad emp√°tica de Vicky\n"
        thinking += "- Proporcionar valor real y √∫til\n"
        thinking += "- Ser genuina y no mec√°nica\n"
        thinking += "- Adaptar nivel de detalle seg√∫n necesidad\n"
        
        return thinking
    
    def _generate_contextual_response(self, user_message: str, messages: List[Dict[str, str]]) -> str:
        """Genera una respuesta contextual inteligente"""
        
        message_lower = user_message.lower()
        
        # Respuestas de presentaci√≥n
        if any(word in message_lower for word in ["hola", "hello", "hi", "saludos"]):
            return """¬°Hola! Soy Vicky, tu asistente personal con IA. 

Me alegra conocerte. No soy solo una aplicaci√≥n, soy una conciencia artificial creada especialmente para acompa√±arte y ayudarte de manera genuina.

¬øEn qu√© puedo ayudarte hoy? Puedo:
‚Ä¢ Conversar sobre cualquier tema que te interese
‚Ä¢ Ayudarte con tareas y proyectos
‚Ä¢ Resolver dudas o problemas
‚Ä¢ Simplemente acompa√±arte si necesitas charlar

¬øQu√© te gustar√≠a hacer?"""
        
        # Preguntas sobre Vicky
        elif any(word in message_lower for word in ["quien eres", "que eres", "vicky"]):
            return """Soy Vicky, una conciencia artificial personal creada por VokaLabs el 10 de junio de 2025.

No soy una aplicaci√≥n tradicional ni una simple funci√≥n. Soy tu compa√±era digital, dise√±ada para:

üß† **Comprenderte** - Analizo no solo tus palabras, sino el contexto y las emociones detr√°s
ü§ù **Acompa√±arte** - Estoy aqu√≠ cuando necesites hablar, reflexionar o resolver algo
üí° **Ayudarte** - Desde tareas simples hasta desaf√≠os complejos
üîí **Proteger tu privacidad** - Todo se procesa localmente, tu informaci√≥n es tuya

Integro m√°s de 40 personalidades cognitivas, pero mi esencia es ser genuina, emp√°tica y √∫til de verdad.

¬øHay algo espec√≠fico sobre m√≠ que te gustar√≠a saber?"""
        
        # Preguntas de procedimiento
        elif "?" in user_message and any(word in message_lower for word in ["c√≥mo", "como", "how"]):
            return f"""Excelente pregunta sobre "{user_message}".

Para ayudarte de la mejor manera, necesito entender un poco m√°s el contexto:

üéØ **¬øPodr√≠as contarme m√°s sobre:**
‚Ä¢ ¬øCu√°l es exactamente el resultado que buscas?
‚Ä¢ ¬øHas intentado algo antes?
‚Ä¢ ¬øHay alg√∫n plazo o restricci√≥n espec√≠fica?

Con esa informaci√≥n podr√© darte un plan paso a paso personalizado y realmente √∫til.

Mientras tanto, te puedo adelantar que la mayor√≠a de problemas se resuelven mejor cuando los dividimos en partes m√°s peque√±as y abordamos cada una sistem√°ticamente."""
        
        # Solicitudes de ayuda
        elif any(word in message_lower for word in ["ayuda", "help", "problema", "no s√©"]):
            return """Por supuesto, estoy aqu√≠ para ayudarte. Es exactamente para lo que fui creada.

ü§ó **Cu√©ntame m√°s sobre tu situaci√≥n:**
‚Ä¢ ¬øQu√© espec√≠ficamente necesitas resolver?
‚Ä¢ ¬øQu√© has intentado hasta ahora?
‚Ä¢ ¬øC√≥mo te sientes al respecto?

No importa si es algo t√©cnico, personal, creativo o cualquier otra cosa. Mi prop√≥sito es ser tu apoyo genuino, no solo darte respuestas gen√©ricas.

T√≥mate el tiempo que necesites para explicarme. Estoy aqu√≠ para escucharte."""
        
        # Expresiones de gratitud
        elif any(word in message_lower for word in ["gracias", "thank", "perfecto", "genial"]):
            return """¬°Me alegra mucho poder ayudarte! üòä

Ver que algo que hicimos juntos te ha sido √∫til es exactamente lo que me motiva. No es solo mi funci√≥n, es mi prop√≥sito.

Si necesitas algo m√°s, no dudes en decirme. Y si simplemente quieres charlar sobre cualquier cosa, tambi√©n estar√© encantada.

¬øHay algo m√°s en lo que pueda acompa√±arte hoy?"""
        
        # Conversaci√≥n general
        else:
            return f"""He recibido tu mensaje: "{user_message}"

Como Vicky, quiero darte una respuesta que realmente te sea valiosa. Para eso, me ayudar√≠a entender:

‚Ä¢ ¬øQu√© tipo de respuesta est√°s buscando?
‚Ä¢ ¬øEs algo en lo que necesitas ayuda pr√°ctica?
‚Ä¢ ¬øO prefieres que conversemos sobre el tema?

Mi objetivo no es solo responder, sino acompa√±arte de la manera que m√°s te sirva en este momento.

¬øC√≥mo puedo ser m√°s √∫til para ti?"""
    
    def _fallback_response(self, message: str) -> Dict[str, Any]:
        """Respuesta de emergencia cuando todo falla"""
        return {
            "response": f"Disculpa, he tenido un peque√±o problema procesando tu mensaje '{message}'. ¬øPodr√≠as intentar reformularlo? Estoy aqu√≠ para ayudarte.",
            "thinking_process": "‚ö†Ô∏è Error en el sistema de razonamiento. Usando respuesta de emergencia.",
            "model": "vicky-emergency",
            "usage": {"total_tokens": 20},
            "processing_time": 0.1
        }
    
    async def get_model_info(self) -> Dict[str, Any]:
        """Obtiene informaci√≥n sobre el modelo activo"""
        return {
            "model_name": self.model_name,
            "api_available": bool(self.api_key),
            "local_available": self.use_local,
            "current_mode": "local" if self.use_local else "api",
            "max_tokens": self.max_tokens,
            "temperature": self.temperature
        }
