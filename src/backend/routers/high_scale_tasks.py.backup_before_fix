#!/usr/bin/env python3
"""
VokaFlow - Router de Tareas de Alta Escala
API optimizada para millones de solicitudes por segundo
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional
from fastapi import APIRouter, HTTPException, Depends, Query, status, BackgroundTasks
from pydantic import BaseModel, Field
from datetime import datetime
import json

from ..core.high_scale_task_manager import (
    high_scale_task_manager,
    TaskPriority,
    WorkerType,
    TaskStatus,
    submit_high_scale_task,
    get_scale_metrics,
    initialize_high_scale_system,
    shutdown_high_scale_system
)

logger = logging.getLogger("vokaflow.high_scale_tasks")

# Router optimizado
router = APIRouter(tags=["High Scale Tasks"])

# Modelos Pydantic optimizados
class HighScaleTaskRequest(BaseModel):
    name: str = Field(..., min_length=1, max_length=100)
    function_name: str = Field(..., min_length=1, max_length=50)
    args: List[Any] = Field(default_factory=list, max_items=20)
    kwargs: Dict[str, Any] = Field(default_factory=dict, max_properties=20)
    priority: str = Field("NORMAL", pattern="^(EMERGENCY|CRITICAL|HIGH|NORMAL|LOW|BATCH|BACKGROUND|MAINTENANCE)$")
    worker_type: str = Field("GENERAL_PURPOSE", pattern="^(CPU_INTENSIVE|IO_INTENSIVE|MEMORY_INTENSIVE|NETWORK_INTENSIVE|GENERAL_PURPOSE)$")
    category: str = Field("general", min_length=1, max_length=50)
    max_retries: int = Field(3, ge=0, le=10)
    timeout: Optional[float] = Field(None, gt=0, le=300)
    rate_limit: Optional[int] = Field(None, gt=0, le=100000)
    estimated_duration: float = Field(1.0, gt=0, le=3600)
    memory_requirement: int = Field(64, ge=1, le=8192)
    cpu_requirement: float = Field(0.1, ge=0.01, le=8.0)

class BatchTaskRequest(BaseModel):
    tasks: List[HighScaleTaskRequest] = Field(..., min_items=1, max_items=1000)
    batch_priority: str = Field("NORMAL")
    execution_mode: str = Field("parallel", pattern="^(parallel|sequential|adaptive)$")

class TaskResponse(BaseModel):
    task_id: str
    name: str
    status: str
    partition: int
    redis_node: str
    estimated_completion: Optional[datetime] = None
    queue_position: Optional[int] = None

class ScaleMetricsResponse(BaseModel):
    throughput_per_second: float
    total_pending_tasks: int
    active_workers: int
    redis_nodes: int
    partitions: int
    system_resources: Dict[str, float]
    worker_pools: Dict[str, Dict[str, Any]]
    queue_distribution: Dict[str, int]
    timestamp: float

class PriorityConfigResponse(BaseModel):
    priority_levels: Dict[str, Dict[str, Any]]
    worker_types: Dict[str, Dict[str, Any]]
    rate_limits: Dict[str, int]
    auto_scaling_enabled: bool
    monitoring_enabled: bool

# Funciones auxiliares optimizadas
REGISTERED_HIGH_SCALE_FUNCTIONS = {
    # Funciones de Vicky AI
    "vicky_inference": lambda prompt, model="qwen_7b": f"Vicky AI procesando: {prompt} con {model}",
    "vicky_embedding": lambda text: f"Embedding generado para: {text[:50]}...",
    "vicky_classification": lambda text, categories: f"Clasificaci√≥n: {text[:30]} -> {categories[0] if categories else 'unknown'}",
    
    # Procesamiento de audio de alta escala
    "audio_transcription": lambda audio_path: f"Transcripci√≥n de audio: {audio_path}",
    "audio_analysis": lambda audio_path, features: f"An√°lisis de audio: {features}",
    "voice_synthesis": lambda text, voice_id: f"S√≠ntesis de voz: {text[:50]}...",
    
    # Operaciones de base de datos
    "bulk_insert": lambda table, data: f"Inserci√≥n masiva en {table}: {len(data) if isinstance(data, list) else 1} registros",
    "data_aggregation": lambda query, timeframe: f"Agregaci√≥n de datos: {query} ({timeframe})",
    "cache_refresh": lambda cache_key: f"Cache actualizado: {cache_key}",
    
    # Notificaciones masivas
    "send_bulk_notifications": lambda users, message: f"Notificaciones enviadas a {len(users) if isinstance(users, list) else 1} usuarios",
    "push_notification": lambda user_id, payload: f"Push enviado a {user_id}",
    "email_batch": lambda recipients, template: f"Emails enviados: {template} a {len(recipients) if isinstance(recipients, list) else 1} destinatarios",
    
    # Procesamiento de archivos
    "file_processing": lambda file_path, operation: f"Procesamiento de archivo: {operation} en {file_path}",
    "image_optimization": lambda image_path, quality: f"Optimizaci√≥n de imagen: {image_path} (calidad: {quality})",
    "video_transcoding": lambda video_path, format: f"Transcodificaci√≥n: {video_path} -> {format}",
    
    # Analytics y m√©tricas
    "metrics_calculation": lambda metric_type, period: f"C√°lculo de m√©tricas: {metric_type} ({period})",
    "report_generation": lambda report_type, data_range: f"Generaci√≥n de reporte: {report_type} ({data_range})",
    "data_backup": lambda source, destination: f"Backup: {source} -> {destination}",
    
    # Machine Learning
    "model_training": lambda model_name, dataset: f"Entrenamiento de modelo: {model_name} con {dataset}",
    "batch_prediction": lambda model, input_data: f"Predicciones en lote: {model} ({len(input_data) if isinstance(input_data, list) else 1} elementos)",
    "feature_extraction": lambda data, features: f"Extracci√≥n de caracter√≠sticas: {features}",
    
    # Tareas de sistema
    "system_health_check": lambda component: f"Verificaci√≥n de salud: {component}",
    "log_analysis": lambda log_file, pattern: f"An√°lisis de logs: {pattern} en {log_file}",
    "cleanup_temp_files": lambda directory: f"Limpieza de archivos temporales: {directory}",
    
    # Comunicaciones en tiempo real
    "websocket_broadcast": lambda channel, message: f"Broadcast WebSocket: {channel}",
    "realtime_sync": lambda user_id, data: f"Sincronizaci√≥n en tiempo real: {user_id}",
    "live_translation": lambda text, source_lang, target_lang: f"Traducci√≥n en vivo: {source_lang} -> {target_lang}"
}

@router.on_event("startup")
async def startup_high_scale_system():
    """Inicializar sistema de alta escala al arrancar"""
    try:
        await initialize_high_scale_system()
        logger.info("üöÄ Sistema de alta escala inicializado")
    except Exception as e:
        logger.error(f"‚ùå Error inicializando sistema de alta escala: {e}")

@router.on_event("shutdown")
async def shutdown_high_scale_system():
    """Shutdown del sistema de alta escala"""
    try:
        await shutdown_high_scale_system()
        logger.info("üõë Sistema de alta escala finalizado")
    except Exception as e:
        logger.error(f"‚ùå Error finalizando sistema de alta escala: {e}")

@router.post("/submit", response_model=TaskResponse)
async def submit_high_performance_task(task_request: HighScaleTaskRequest):
    """
    üöÄ Enviar tarea optimizada para alta escala
    
    Optimizado para millones de solicitudes por segundo:
    - Rate limiting autom√°tico
    - Distribuci√≥n por particiones
    - Workers especializados
    - Prioridades granulares
    """
    try:
        # Validar funci√≥n
        if task_request.function_name not in REGISTERED_HIGH_SCALE_FUNCTIONS:
            available_functions = list(REGISTERED_HIGH_SCALE_FUNCTIONS.keys())
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Funci√≥n '{task_request.function_name}' no registrada. "
                       f"Disponibles: {available_functions[:10]}..."  # Mostrar solo las primeras 10
            )
        
        # Convertir enums
        try:
            priority = TaskPriority[task_request.priority]
            worker_type = WorkerType[task_request.worker_type]
        except KeyError as e:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Valor inv√°lido: {e}"
            )
        
        # Obtener funci√≥n
        func = REGISTERED_HIGH_SCALE_FUNCTIONS[task_request.function_name]
        
        # Enviar tarea con par√°metros optimizados
        task_id = await submit_high_scale_task(
            func=func,
            args=tuple(task_request.args),
            kwargs=task_request.kwargs,
            priority=priority,
            worker_type=worker_type,
            name=task_request.name,
            category=task_request.category,
            max_retries=task_request.max_retries,
            timeout=task_request.timeout,
            rate_limit=task_request.rate_limit,
            estimated_duration=task_request.estimated_duration,
            memory_requirement=task_request.memory_requirement,
            cpu_requirement=task_request.cpu_requirement
        )
        
        # Calcular informaci√≥n de partici√≥n y posici√≥n
        import hashlib
        partition = int(hashlib.md5(task_id.encode()).hexdigest(), 16) % 16
        redis_node = f"node_{partition % 3}"  # Asumiendo 3 nodos Redis
        
        return TaskResponse(
            task_id=task_id,
            name=task_request.name,
            status="queued",
            partition=partition,
            redis_node=redis_node,
            estimated_completion=None,  # Se calcular√≠a en producci√≥n
            queue_position=None  # Se calcular√≠a en producci√≥n
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error enviando tarea de alta escala: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error interno: {str(e)}"
        )

@router.post("/batch", response_model=Dict[str, Any])
async def submit_batch_high_scale_tasks(batch_request: BatchTaskRequest):
    """
    üì¶ Enviar lote de tareas optimizado para alta escala
    
    Caracter√≠sticas:
    - Procesamiento paralelo de env√≠os
    - Validaci√≥n en lote
    - Distribuci√≥n inteligente
    - Manejo de errores granular
    """
    try:
        if len(batch_request.tasks) > 1000:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="M√°ximo 1000 tareas por lote"
            )
        
        submitted_tasks = []
        errors = []
        
        # Procesar tareas en paralelo (grupos de 100)
        batch_size = 100
        for i in range(0, len(batch_request.tasks), batch_size):
            batch_chunk = batch_request.tasks[i:i + batch_size]
            
            # Crear tareas async para cada chunk
            async_tasks = []
            for j, task_request in enumerate(batch_chunk):
                try:
                    # Validar funci√≥n
                    if task_request.function_name not in REGISTERED_HIGH_SCALE_FUNCTIONS:
                        errors.append({
                            "index": i + j,
                            "error": f"Funci√≥n '{task_request.function_name}' no registrada"
                        })
                        continue
                    
                    # Convertir enums
                    priority = TaskPriority[task_request.priority]
                    worker_type = WorkerType[task_request.worker_type]
                    func = REGISTERED_HIGH_SCALE_FUNCTIONS[task_request.function_name]
                    
                    # Crear tarea async
                    task_coroutine = submit_high_scale_task(
                        func=func,
                        args=tuple(task_request.args),
                        kwargs=task_request.kwargs,
                        priority=priority,
                        worker_type=worker_type,
                        name=task_request.name,
                        category=task_request.category,
                        max_retries=task_request.max_retries,
                        timeout=task_request.timeout
                    )
                    async_tasks.append((i + j, task_coroutine, task_request.name))
                    
                except Exception as e:
                    errors.append({
                        "index": i + j,
                        "error": str(e)
                    })
            
            # Ejecutar chunk en paralelo
            if async_tasks:
                chunk_results = await asyncio.gather(
                    *[task_coro for _, task_coro, _ in async_tasks],
                    return_exceptions=True
                )
                
                for (index, _, name), result in zip(async_tasks, chunk_results):
                    if isinstance(result, Exception):
                        errors.append({
                            "index": index,
                            "error": str(result)
                        })
                    else:
                        submitted_tasks.append({
                            "index": index,
                            "task_id": result,
                            "name": name,
                            "status": "submitted"
                        })
        
        return {
            "submitted_tasks": submitted_tasks,
            "total_submitted": len(submitted_tasks),
            "errors": errors,
            "total_errors": len(errors),
            "execution_mode": batch_request.execution_mode,
            "batch_priority": batch_request.batch_priority,
            "processing_time": "optimized_parallel"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error enviando lote de tareas de alta escala: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error procesando lote: {str(e)}"
        )

@router.get("/metrics", response_model=ScaleMetricsResponse)
async def get_high_scale_metrics():
    """
    üìä Obtener m√©tricas del sistema de alta escala
    
    M√©tricas optimizadas para monitoreo en tiempo real:
    - Throughput por segundo
    - Distribuci√≥n de colas
    - Recursos del sistema
    - Estado de workers
    """
    try:
        metrics = await get_scale_metrics()
        
        # Procesar m√©tricas para respuesta optimizada
        queue_distribution = {}
        if "queue_lengths" in metrics:
            # Agrupar por prioridad
            for queue_key, length in metrics["queue_lengths"].items():
                priority = queue_key.split(":")[2] if ":" in queue_key else "unknown"
                queue_distribution[priority] = queue_distribution.get(priority, 0) + length
        
        return ScaleMetricsResponse(
            throughput_per_second=metrics.get("throughput_per_second", 0.0),
            total_pending_tasks=metrics.get("total_pending_tasks", 0),
            active_workers=metrics.get("active_workers", 0),
            redis_nodes=metrics.get("redis_nodes", 0),
            partitions=metrics.get("partitions", 0),
            system_resources=metrics.get("system_resources", {}),
            worker_pools=metrics.get("worker_pools", {}),
            queue_distribution=queue_distribution,
            timestamp=metrics.get("timestamp", 0.0)
        )
        
    except Exception as e:
        logger.error(f"Error obteniendo m√©tricas de alta escala: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error obteniendo m√©tricas: {str(e)}"
        )

@router.get("/priorities", response_model=PriorityConfigResponse)
async def get_priority_configuration():
    """
    ‚öôÔ∏è Obtener configuraci√≥n de prioridades y workers
    
    Informaci√≥n sobre:
    - Niveles de prioridad disponibles
    - Tipos de workers especializados
    - Rate limits por categor√≠a
    - Configuraci√≥n de auto-scaling
    """
    try:
        priority_levels = {
            priority.name: {
                "value": priority.value,
                "description": _get_priority_description(priority),
                "typical_use_cases": _get_priority_use_cases(priority),
                "sla_target": _get_priority_sla(priority)
            }
            for priority in TaskPriority
        }
        
        worker_types = {
            worker_type.value: {
                "description": _get_worker_type_description(worker_type),
                "optimal_for": _get_worker_type_optimal_for(worker_type),
                "max_workers": high_scale_task_manager.max_workers_per_type.get(worker_type, 0),
                "pool_type": "ProcessPool" if worker_type == WorkerType.CPU_INTENSIVE else "ThreadPool"
            }
            for worker_type in WorkerType
        }
        
        # Rate limits por categor√≠a (valores por defecto)
        rate_limits = {
            "vicky": 50000,      # 50K req/s para Vicky AI
            "audio": 20000,      # 20K req/s para procesamiento de audio
            "database": 100000,  # 100K req/s para operaciones de BD
            "notifications": 1000000,  # 1M req/s para notificaciones
            "analytics": 30000,  # 30K req/s para analytics
            "ml": 10000,         # 10K req/s para ML
            "system": 5000,      # 5K req/s para tareas de sistema
            "general": 10000     # 10K req/s por defecto
        }
        
        return PriorityConfigResponse(
            priority_levels=priority_levels,
            worker_types=worker_types,
            rate_limits=rate_limits,
            auto_scaling_enabled=high_scale_task_manager.enable_auto_scaling,
            monitoring_enabled=high_scale_task_manager.enable_monitoring
        )
        
    except Exception as e:
        logger.error(f"Error obteniendo configuraci√≥n: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error obteniendo configuraci√≥n: {str(e)}"
        )

@router.get("/functions", response_model=Dict[str, Any])
async def get_high_scale_functions():
    """
    üîß Listar funciones disponibles para alta escala
    
    Funciones optimizadas por categor√≠a:
    - Vicky AI
    - Procesamiento de audio
    - Base de datos
    - Notificaciones
    - Machine Learning
    - Sistema
    """
    try:
        functions_by_category = {
            "vicky_ai": [func for func in REGISTERED_HIGH_SCALE_FUNCTIONS.keys() if func.startswith("vicky_")],
            "audio": [func for func in REGISTERED_HIGH_SCALE_FUNCTIONS.keys() if "audio" in func or "voice" in func],
            "database": [func for func in REGISTERED_HIGH_SCALE_FUNCTIONS.keys() if any(x in func for x in ["bulk_", "data_", "cache_"])],
            "notifications": [func for func in REGISTERED_HIGH_SCALE_FUNCTIONS.keys() if any(x in func for x in ["notification", "email", "push"])],
            "files": [func for func in REGISTERED_HIGH_SCALE_FUNCTIONS.keys() if any(x in func for x in ["file_", "image_", "video_"])],
            "analytics": [func for func in REGISTERED_HIGH_SCALE_FUNCTIONS.keys() if any(x in func for x in ["metrics_", "report_", "backup"])],
            "ml": [func for func in REGISTERED_HIGH_SCALE_FUNCTIONS.keys() if any(x in func for x in ["model_", "batch_prediction", "feature_"])],
            "system": [func for func in REGISTERED_HIGH_SCALE_FUNCTIONS.keys() if any(x in func for x in ["system_", "log_", "cleanup_"])],
            "realtime": [func for func in REGISTERED_HIGH_SCALE_FUNCTIONS.keys() if any(x in func for x in ["websocket_", "realtime_", "live_"])]
        }
        
        return {
            "functions_by_category": functions_by_category,
            "total_functions": len(REGISTERED_HIGH_SCALE_FUNCTIONS),
            "categories": list(functions_by_category.keys()),
            "recommended_priorities": {
                "vicky_ai": "CRITICAL",
                "audio": "HIGH", 
                "database": "NORMAL",
                "notifications": "HIGH",
                "files": "NORMAL",
                "analytics": "LOW",
                "ml": "BATCH",
                "system": "MAINTENANCE",
                "realtime": "CRITICAL"
            }
        }
        
    except Exception as e:
        logger.error(f"Error listando funciones de alta escala: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error listando funciones: {str(e)}"
        )

# Funciones auxiliares
def _get_priority_description(priority: TaskPriority) -> str:
    descriptions = {
        TaskPriority.EMERGENCY: "Emergencias cr√≠ticas que requieren atenci√≥n inmediata",
        TaskPriority.CRITICAL: "Tareas cr√≠ticas del sistema con SLA estricto",
        TaskPriority.HIGH: "Alta prioridad para operaciones importantes",
        TaskPriority.NORMAL: "Prioridad normal para operaciones est√°ndar",
        TaskPriority.LOW: "Baja prioridad para tareas no urgentes",
        TaskPriority.BATCH: "Procesamiento en lotes optimizado",
        TaskPriority.BACKGROUND: "Tareas de fondo sin urgencia",
        TaskPriority.MAINTENANCE: "Tareas de mantenimiento del sistema"
    }
    return descriptions.get(priority, "Sin descripci√≥n")

def _get_priority_use_cases(priority: TaskPriority) -> List[str]:
    use_cases = {
        TaskPriority.EMERGENCY: ["Fallos cr√≠ticos", "Alertas de seguridad", "Recuperaci√≥n de desastres"],
        TaskPriority.CRITICAL: ["Procesamiento de Vicky AI", "Transacciones financieras", "Comunicaciones en tiempo real"],
        TaskPriority.HIGH: ["An√°lisis de audio", "Notificaciones push", "Consultas interactivas"],
        TaskPriority.NORMAL: ["Procesamiento de archivos", "Reportes", "Operaciones CRUD"],
        TaskPriority.LOW: ["Analytics", "Agregaciones", "Sincronizaci√≥n"],
        TaskPriority.BATCH: ["ML training", "Procesamiento masivo", "ETL"],
        TaskPriority.BACKGROUND: ["Limpieza", "Optimizaci√≥n", "Indexaci√≥n"],
        TaskPriority.MAINTENANCE: ["Backups", "Health checks", "Log rotation"]
    }
    return use_cases.get(priority, [])

def _get_priority_sla(priority: TaskPriority) -> str:
    slas = {
        TaskPriority.EMERGENCY: "< 100ms",
        TaskPriority.CRITICAL: "< 500ms",
        TaskPriority.HIGH: "< 2s",
        TaskPriority.NORMAL: "< 10s",
        TaskPriority.LOW: "< 60s",
        TaskPriority.BATCH: "< 5min",
        TaskPriority.BACKGROUND: "< 30min",
        TaskPriority.MAINTENANCE: "< 2h"
    }
    return slas.get(priority, "Sin SLA definido")

def _get_worker_type_description(worker_type: WorkerType) -> str:
    descriptions = {
        WorkerType.CPU_INTENSIVE: "Workers optimizados para tareas que requieren mucho procesamiento CPU",
        WorkerType.IO_INTENSIVE: "Workers optimizados para operaciones de E/S intensivas",
        WorkerType.MEMORY_INTENSIVE: "Workers optimizados para tareas que requieren mucha memoria",
        WorkerType.NETWORK_INTENSIVE: "Workers optimizados para operaciones de red intensivas",
        WorkerType.GENERAL_PURPOSE: "Workers de prop√≥sito general para tareas variadas"
    }
    return descriptions.get(worker_type, "Sin descripci√≥n")

def _get_worker_type_optimal_for(worker_type: WorkerType) -> List[str]:
    optimal_for = {
        WorkerType.CPU_INTENSIVE: ["ML training", "C√°lculos complejos", "Procesamiento de im√°genes", "Algoritmos"],
        WorkerType.IO_INTENSIVE: ["Operaciones de BD", "Lectura/escritura de archivos", "Logs", "Cache"],
        WorkerType.MEMORY_INTENSIVE: ["Procesamiento de grandes datasets", "Analytics en memoria", "Caching"],
        WorkerType.NETWORK_INTENSIVE: ["APIs externas", "Webhooks", "Notificaciones", "Sincronizaci√≥n"],
        WorkerType.GENERAL_PURPOSE: ["Tareas mixtas", "Procesamiento general", "CRUD", "Validaciones"]
    }
    return optimal_for.get(worker_type, []) 


# ===========================================
# NEW APIS - DLQ Y DISTRIBUTED LOCKING
# ===========================================

@router.get("/dlq", response_model=Dict[str, Any])
async def get_dead_letter_queue_tasks(
    worker_type: Optional[str] = Query(None, description="Filtrar por tipo de worker"),
    limit: int = Query(100, ge=1, le=1000, description="L√≠mite de tareas a retornar")
):
    """
    üìã Obtener tareas de Dead Letter Queue
    
    Retorna tareas que fallaron despu√©s de agotar todos los reintentos:
    - Tareas con errores permanentes
    - Tareas que excedieron max_retries
    - Informaci√≥n detallada de errores
    """
    try:
        if not high_scale_task_manager:
            await initialize_high_scale_system()
        
        dlq_tasks = await high_scale_task_manager.get_dlq_tasks(worker_type, limit)
        
        return {
            "total_tasks": len(dlq_tasks),
            "worker_type_filter": worker_type,
            "tasks": dlq_tasks,
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"Error obteniendo DLQ: {e}")
        raise HTTPException(status_code=500, detail=f"Error obteniendo DLQ: {str(e)}")


@router.post("/dlq/{task_id}/retry")
async def retry_dlq_task(task_id: str):
    """
    üîÑ Reintentar tarea desde Dead Letter Queue
    
    Remueve la tarea de DLQ y la reencola para procesamiento:
    - Resetea contadores de reintentos
    - Elimina informaci√≥n de error
    - Reencola con prioridad original
    """
    try:
        if not high_scale_task_manager:
            await initialize_high_scale_system()
        
        success = await high_scale_task_manager.retry_dlq_task(task_id)
        
        if success:
            return {
                "success": True,
                "message": f"Tarea {task_id} reintentada desde DLQ",
                "task_id": task_id,
                "timestamp": time.time()
            }
        else:
            raise HTTPException(status_code=404, detail=f"Tarea {task_id} no encontrada en DLQ")
            
    except Exception as e:
        logger.error(f"Error reintentando tarea DLQ {task_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Error reintentando tarea: {str(e)}")


@router.post("/lock/{lock_key}/acquire")
async def acquire_distributed_lock(
    lock_key: str,
    worker_id: str = Query(..., description="ID √∫nico del worker solicitante"),
    timeout: int = Query(30, ge=1, le=300, description="Timeout en segundos")
):
    """
    üîí Adquirir lock distribuido
    
    Permite coordinaci√≥n entre workers distribuidos:
    - Lock at√≥mico usando Redis
    - Expiraci√≥n autom√°tica
    - Previene procesamiento duplicado
    """
    try:
        if not high_scale_task_manager:
            await initialize_high_scale_system()
        
        acquired = await high_scale_task_manager.acquire_distributed_lock(lock_key, worker_id, timeout)
        
        return {
            "acquired": acquired,
            "lock_key": lock_key,
            "worker_id": worker_id,
            "timeout": timeout,
            "expires_at": time.time() + timeout if acquired else None,
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"Error adquiriendo lock {lock_key}: {e}")
        raise HTTPException(status_code=500, detail=f"Error adquiriendo lock: {str(e)}")


@router.delete("/lock/{lock_key}/release")
async def release_distributed_lock(
    lock_key: str,
    worker_id: str = Query(..., description="ID del worker que posee el lock")
):
    """
    üîì Liberar lock distribuido
    
    Libera lock previamente adquirido:
    - Verificaci√≥n de ownership
    - Release at√≥mico
    - Logging de operaci√≥n
    """
    try:
        if not high_scale_task_manager:
            await initialize_high_scale_system()
        
        released = await high_scale_task_manager.release_distributed_lock(lock_key, worker_id)
        
        return {
            "released": released,
            "lock_key": lock_key,
            "worker_id": worker_id,
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"Error liberando lock {lock_key}: {e}")
        raise HTTPException(status_code=500, detail=f"Error liberando lock: {str(e)}")


@router.get("/workers/status")
async def get_workers_status():
    """
    üë∑ Estado de workers distribuidos
    
    Informaci√≥n detallada de workers Redis:
    - Workers activos por tipo
    - Colas monitoreadas
    - M√©tricas de rendimiento
    """
    try:
        if not high_scale_task_manager:
            await initialize_high_scale_system()
        
        metrics = await high_scale_task_manager.get_global_metrics()
        
        worker_status = {
            "total_workers": sum(metrics["worker_pools"][wt]["max_workers"] for wt in metrics["worker_pools"]),
            "active_workers": metrics.get("active_workers", 0),
            "worker_pools": metrics["worker_pools"],
            "redis_nodes": metrics.get("redis_nodes", 0),
            "partitions": metrics.get("partitions", 0),
            "system_resources": metrics.get("system_resources", {}),
            "timestamp": time.time()
        }
        
        # Agregar informaci√≥n de workers Redis si est√°n disponibles
        if hasattr(high_scale_task_manager, 'worker_tasks'):
            worker_status["redis_workers"] = {
                "total": len(high_scale_task_manager.worker_tasks),
                "running": len([t for t in high_scale_task_manager.worker_tasks if not t.done()]),
                "failed": len([t for t in high_scale_task_manager.worker_tasks if t.done() and t.exception()])
            }
        
        return worker_status
        
    except Exception as e:
        logger.error(f"Error obteniendo estado de workers: {e}")
        raise HTTPException(status_code=500, detail=f"Error obteniendo estado: {str(e)}") 